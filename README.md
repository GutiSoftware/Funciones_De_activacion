# Funciones_De_activacion
Comparación de funciones de activación nuevas (Custom) con funciones de activacion estandard.

 El script principal (Funciones_Activacion.py está diseñado para comparar diferentes funciones de activación en una red
 neuronal convolucional (CNN) utilizando el conjunto de datos MNIST de dígitos escritos a
 mano. El objetivo es evaluar el rendimiento de las funciones de activación estandard ReLU, Sigmoid y Tanh, con funciones de
 activación personalizadas (Custom), guardando los resultados en unos archivos gráficos .png y .JSON en una
 carpeta llamada "Resultados"
 El Script: Tabla_Comparativa_Modelos.py hace una comparacion de los resultados obtenidos por el script anterior
 en guardando los resultados en formato .csv
